{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8434634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ffcfe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = './assets/im1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "dde15e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxAndLineOverlap(x_mid_point, y_mid_point, line_coordinates):\n",
    "    x1_line, y1_line, x2_line, y2_line = line_coordinates #Unpacking\n",
    "\n",
    "    if (x_mid_point >= x1_line and x_mid_point <= x2_line+5) and\\\n",
    "        (y_mid_point >= y1_line and y_mid_point <= y2_line+5):\n",
    "        return True\n",
    "    return False\n",
    "def boxAndAreaOverlap(x_mid_point, y_mid_point, line_coordinates):\n",
    "    x1_line, y1_line, x2_line, y2_line = line_coordinates #Unpacking\n",
    "\n",
    "    if (x_mid_point >= x1_line and x_mid_point <= x2_line) and\\\n",
    "        (y_mid_point >= y1_line and y_mid_point <= y2_line):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "50de0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(os.getcwd(), 'assets', 'people.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "x1_area = int(0.10 * video_width)\n",
    "y1_area = int(0.10 * video_height)\n",
    "x2_area = int(0.90 * video_width)\n",
    "y2_area = int(0.90 * video_height)\n",
    "\n",
    "detections = []\n",
    "def defun(cap):\n",
    "\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    results = model(frame)\n",
    "    \n",
    "    detections = []\n",
    "    for result in results:\n",
    "        for r in result.boxes.data.tolist():\n",
    "            x1, y1, x2, y2, score, class_id = r\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "            score = float(score)\n",
    "            class_id = int(class_id)\n",
    "\n",
    "            if score > 0.5 and class_id == 0:\n",
    "                detections.append([x1, y1, x2, y2, score, class_id])\n",
    "    return ret, frame, detections\n",
    "# for i in range(5):\n",
    "#     defun()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92978588",
   "metadata": {},
   "source": [
    "defun(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709b89b",
   "metadata": {},
   "source": [
    "\n",
    "video_path = os.path.join(os.getcwd(), 'assets', 'people.mp4')\n",
    "video_out_path = os.path.join(os.getcwd(), 'assets', 'people_out.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "cap_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'mp4v'), cap.get(cv2.CAP_PROP_FPS), (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "tracker = Tracker()\n",
    "\n",
    "colors = [(random.randint(0,255), random.randint(0,255), random.randint(0,255)) for j in range(100)]\n",
    "while ret:\n",
    "\n",
    "    results = model(frame)\n",
    "    for result in results:\n",
    "        detections = []\n",
    "        for r in result.boxes.data.tolist():\n",
    "            x1, y1, x2, y2, score, class_id = r\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "            score = float(score)\n",
    "            class_id = int(class_id)\n",
    "\n",
    "            if score > 0.5:\n",
    "                detections.append([x1, y1, x2, y2, score, class_id])\n",
    "\n",
    "\n",
    "    tracker.update(frame, detections)\n",
    "    \n",
    "    for track in tracker.tracks:\n",
    "        bbox = track.bbox\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        track_id = track.track_id\n",
    "        # class_id = track.class_id\n",
    "        # score = track.score\n",
    "\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), colors[track_id%len(colors)], 2)\n",
    "        cv2.putText(frame, f'{track_id}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, colors[track_id%len(colors)], 2)\n",
    "    \n",
    "    # cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    # cap_out.write(frame)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "7481a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.deep_sort.tracker import Tracker as DeepSortTracker\n",
    "from deep_sort.tools import generate_detections as gdet\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    tracker = None\n",
    "    encoder = None\n",
    "    tracks = None\n",
    "    raw_tracks = None\n",
    "\n",
    "    def __init__(self):\n",
    "        max_cosine_distance = 0.4\n",
    "        nn_budget = None\n",
    "\n",
    "        encoder_model_filename = 'model_data/mars-small128.pb'\n",
    "\n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "        self.tracker = DeepSortTracker(metric)\n",
    "        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)\n",
    "\n",
    "    def update(self, frame, detections):\n",
    "\n",
    "        bboxes = np.asarray([d[:-2] for d in detections])\n",
    "        bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\n",
    "#         Yolo_confidence_scores = [d[-2] for d in detections]\n",
    "        scores = [d[-2] for d in detections]\n",
    "        \n",
    "        features = self.encoder(frame, bboxes)\n",
    "\n",
    "        dets = []\n",
    "        for bbox_id, bbox in enumerate(bboxes):\n",
    "            dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\n",
    "            \n",
    "\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(dets)\n",
    "        self.update_tracks()\n",
    "\n",
    "    def update_tracks(self):\n",
    "        tracks = []\n",
    "#         raw_tracks = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()\n",
    "\n",
    "            id = track.track_id\n",
    "\n",
    "#             tracks.append(Track(id, bbox,0))\n",
    "#             raw_tracks.append(track)\n",
    "            tracks.append(track)\n",
    "\n",
    "        self.tracks = tracks\n",
    "#         self.raw_tracks = raw_tracks\n",
    "\n",
    "\n",
    "# class Track:\n",
    "#     track_id = None\n",
    "#     bbox = None\n",
    "#     class_id = None\n",
    "# #     score = None\n",
    "\n",
    "#     def __init__(self, id, bbox, class_id):\n",
    "#         self.track_id = id\n",
    "#         self.bbox = bbox\n",
    "#         self.class_id = class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f536f",
   "metadata": {},
   "source": [
    "from deep_sort.deep_sort.tracker import Tracker as DeepSortTracker\n",
    "from deep_sort.tools import generate_detections as gdet\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b05600",
   "metadata": {},
   "source": [
    "max_cosine_distance = 0.4\n",
    "nn_budget = None\n",
    "encoder_model_filename = 'model_data/mars-small128.pb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b8744",
   "metadata": {},
   "source": [
    "frame, detections = defun(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c56091f",
   "metadata": {},
   "source": [
    "np.asarray([d[:-2] for d in detections])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b404d",
   "metadata": {},
   "source": [
    "bboxes = np.asarray([d[:-2] for d in detections])\n",
    "print(bboxes)\n",
    "bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\n",
    "bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768ee20",
   "metadata": {},
   "source": [
    "max_cosine_distance = 0.4\n",
    "nn_budget = None\n",
    "\n",
    "encoder_model_filename = 'model_data/mars-small128.pb'\n",
    "\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "tracker = DeepSortTracker(metric)\n",
    "encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edf430",
   "metadata": {},
   "source": [
    "scores = [d[-2] for d in detections]\n",
    "\n",
    "features = encoder(frame, bboxes)\n",
    "\n",
    "dets = []\n",
    "for bbox_id, bbox in enumerate(bboxes):\n",
    "    dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\n",
    "\n",
    "\n",
    "tracker.predict()\n",
    "tracker.update(dets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c86d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e44df4b",
   "metadata": {},
   "source": [
    "for bbox_id, bbox in enumerate(bboxes):\n",
    "    print(bbox_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5386430",
   "metadata": {},
   "source": [
    "classDict = [d[-1] for d in detections]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eea115",
   "metadata": {},
   "source": [
    "tracks = []\n",
    "for track in tracker.tracks:\n",
    "    if not track.is_confirmed() or track.time_since_update > 1:\n",
    "        continue\n",
    "    bbox = track.to_tlbr()\n",
    "\n",
    "    id = track.track_id\n",
    "\n",
    "    tracks.append(Track(id, bbox,0))\n",
    "\n",
    "tracks = tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5f726",
   "metadata": {},
   "source": [
    "for i in tracks:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf54e3",
   "metadata": {},
   "source": [
    "tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "01892fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "547dac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 33 persons, 1 handbag, 114.4ms\n",
      "Speed: 1.7ms preprocess, 114.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 dog, 128.1ms\n",
      "Speed: 1.7ms preprocess, 128.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 122.6ms\n",
      "Speed: 1.5ms preprocess, 122.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    ret, frame, detections = defun(cap)\n",
    "    tracker.update(frame, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a84ef6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 30 persons, 1 dog, 168.1ms\n",
      "Speed: 1.6ms preprocess, 168.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "ret, frame, detections = defun(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "98f03f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.update(frame, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "6b0de612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     654.15      637.39      706.68         720]\n",
      "[     1072.6      205.24      1116.4      308.77]\n",
      "[     55.196      632.89      116.57      719.77]\n",
      "[     803.73      471.21      855.16      590.87]\n",
      "[     358.59      350.47      411.91      461.69]\n",
      "[     35.722         407      98.009      537.72]\n",
      "[     1174.9      192.54      1232.2      300.82]\n",
      "[     603.38      272.17      648.69      376.38]\n",
      "[     706.39      290.44       752.6      395.72]\n",
      "[     668.81      421.86       708.1       528.7]\n",
      "[        780      148.12      815.55      234.17]\n",
      "[     531.41          28      558.64      112.05]\n",
      "[     459.83      522.25      520.95      667.18]\n",
      "[      746.3      148.66      777.67      231.21]\n",
      "[     456.14      154.73      494.79       259.8]\n",
      "[      371.5      267.68      413.17      358.02]\n",
      "[     918.64          20       944.6         110]\n",
      "[     611.24      57.829      640.81      145.93]\n",
      "[     478.29      102.37      508.84      191.33]\n",
      "[     149.61           0      201.05          96]\n"
     ]
    }
   ],
   "source": [
    "for track in tracker.tracks:\n",
    "    print(track.to_tlbr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c49235a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1071, 202, 1115, 308, 0.824573278427124, 0],\n",
       " [39, 413, 101, 544, 0.8109283447265625, 0],\n",
       " [458, 516, 526, 647, 0.787870466709137, 0],\n",
       " [704, 287, 756, 396, 0.7685851454734802, 0],\n",
       " [645, 644, 719, 720, 0.7617627382278442, 0],\n",
       " [1179, 198, 1230, 308, 0.7565044164657593, 0],\n",
       " [46, 642, 128, 720, 0.7553132176399231, 0],\n",
       " [360, 356, 410, 466, 0.7356264591217041, 0],\n",
       " [602, 277, 649, 383, 0.730269730091095, 0],\n",
       " [801, 464, 860, 586, 0.7194649577140808, 0],\n",
       " [457, 151, 493, 250, 0.7012616395950317, 0],\n",
       " [781, 144, 817, 228, 0.6636248826980591, 0],\n",
       " [672, 421, 707, 529, 0.6387720108032227, 0],\n",
       " [613, 61, 640, 151, 0.6219330430030823, 0],\n",
       " [746, 152, 781, 234, 0.6144081354141235, 0],\n",
       " [477, 100, 508, 185, 0.6012713313102722, 0],\n",
       " [919, 20, 945, 110, 0.5962958931922913, 0],\n",
       " [370, 271, 412, 364, 0.5945473909378052, 0],\n",
       " [532, 27, 558, 112, 0.5792855620384216, 0],\n",
       " [145, 0, 209, 96, 0.5168732404708862, 0],\n",
       " [573, 29, 600, 116, 0.5127284526824951, 0]]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "d32d60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = np.asarray([d[:-2] for d in detections])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "11789b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1071,  202, 1115,  308],\n",
       "       [  39,  413,  101,  544],\n",
       "       [ 458,  516,  526,  647],\n",
       "       [ 704,  287,  756,  396],\n",
       "       [ 645,  644,  719,  720],\n",
       "       [1179,  198, 1230,  308],\n",
       "       [  46,  642,  128,  720],\n",
       "       [ 360,  356,  410,  466],\n",
       "       [ 602,  277,  649,  383],\n",
       "       [ 801,  464,  860,  586],\n",
       "       [ 457,  151,  493,  250],\n",
       "       [ 781,  144,  817,  228],\n",
       "       [ 672,  421,  707,  529],\n",
       "       [ 613,   61,  640,  151],\n",
       "       [ 746,  152,  781,  234],\n",
       "       [ 477,  100,  508,  185],\n",
       "       [ 919,   20,  945,  110],\n",
       "       [ 370,  271,  412,  364],\n",
       "       [ 532,   27,  558,  112],\n",
       "       [ 145,    0,  209,   96],\n",
       "       [ 573,   29,  600,  116]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c95a9eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44, 106],\n",
       "       [ 62, 131],\n",
       "       [ 68, 131],\n",
       "       [ 52, 109],\n",
       "       [ 74,  76],\n",
       "       [ 51, 110],\n",
       "       [ 82,  78],\n",
       "       [ 50, 110],\n",
       "       [ 47, 106],\n",
       "       [ 59, 122],\n",
       "       [ 36,  99],\n",
       "       [ 36,  84],\n",
       "       [ 35, 108],\n",
       "       [ 27,  90],\n",
       "       [ 35,  82],\n",
       "       [ 31,  85],\n",
       "       [ 26,  90],\n",
       "       [ 42,  93],\n",
       "       [ 26,  85],\n",
       "       [ 64,  96],\n",
       "       [ 27,  87]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes[:, 2:] - bboxes[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8fccb4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1115,  308],\n",
       "       [ 101,  544],\n",
       "       [ 526,  647],\n",
       "       [ 756,  396],\n",
       "       [ 719,  720],\n",
       "       [1230,  308],\n",
       "       [ 128,  720],\n",
       "       [ 410,  466],\n",
       "       [ 649,  383],\n",
       "       [ 860,  586],\n",
       "       [ 493,  250],\n",
       "       [ 817,  228],\n",
       "       [ 707,  529],\n",
       "       [ 640,  151],\n",
       "       [ 781,  234],\n",
       "       [ 508,  185],\n",
       "       [ 945,  110],\n",
       "       [ 412,  364],\n",
       "       [ 558,  112],\n",
       "       [ 209,   96],\n",
       "       [ 600,  116]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "70814b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<deep_sort.deep_sort.track.Track at 0x292c3f290>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2c890>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2ed10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f450>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f510>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f5d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f1d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f390>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2c350>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2ecd0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2cf50>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc50f10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc514d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc50910>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53c90>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53ad0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53d10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53410>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53790>,\n",
       " <deep_sort.deep_sort.track.Track at 0x29396ad10>]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "838b2b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tracker.tracks[0].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f6acaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a, h, vx, vy, va, vh = tracker.tracks[0].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6260d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = tracker.tracks[0].mean[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4d39e2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005.7838186569628, 376.0567438971888)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "03797aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      978.6,      320.12,        1033,         432])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker.tracks[0].to_tlbr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2d796503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "07bf174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for track in tracker.tracks:\n",
    "    print(track in tracker.tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0b6acbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Frame:\n",
    "    \"\"\"\n",
    "    This class is used to store the state of the frame\n",
    "    State 0 means that the frame is not processed\n",
    "    State 1 means that the frame is processed\n",
    "    if the frame is not processed there wont be any tracks and only boxes will be present \n",
    "    and we will store the detections in place of tracks\n",
    "\n",
    "\n",
    "    if any object goes undetected in current frame then \n",
    "    the last tracks_status of the object can give us the number of frames required for object to be considered as lost out of area\n",
    "    once the object is lost we no more need to track it and count it\n",
    "    but if the object is not lost it should be kept in counting if it comes back in tracking and is within area.\n",
    "\n",
    "    \"\"\"\n",
    "    # track_id = None\n",
    "    frame_state = None\n",
    "    tracks = None\n",
    "    tracks_state = None\n",
    "    # boxes = None\n",
    "\n",
    "    def __init__(self, frame_state = 0, prev_frame = None):\n",
    "        # if prev_frame is None:\n",
    "        self.frame_state = frame_state\n",
    "        self.tracks = []\n",
    "        # if self.frame_state == 1:\n",
    "        self.tracks_state = dict()\n",
    "        # else:\n",
    "        #     self.frame_state = prev_frame.frame_state\n",
    "        #     self.tracks = prev_frame.tracks\n",
    "        #     tracks_state = prev_frame.tracks_state\n",
    "        # self.boxes = []\n",
    "    \n",
    "    def append(self, track):\n",
    "        self.tracks.append(track)\n",
    "    \n",
    "    def update(self, tracks):\n",
    "        untracked_tracks = list(set(self.tracks) - set(tracks))\n",
    "        \n",
    "        for track in tracks:\n",
    "            if track not in self.tracks:\n",
    "                self.append(track)\n",
    "                # self.update_state(track.track_id, 0)\n",
    "            state = self.calculate_state(track)\n",
    "            self.update_state(track.track_id, state)\n",
    "            \n",
    "        for track in untracked_tracks:\n",
    "            print(self.tracks_state[track.track_id][1])\n",
    "            self.tracks_state[track.track_id][1] -= 1\n",
    "            print(self.tracks_state[track.track_id][1])\n",
    "            \n",
    "            if self.tracks_state[track.track_id][1] <= 0:\n",
    "                self.tracks.remove(track)\n",
    "\n",
    "\n",
    "    def calculate_state(self, track):\n",
    "        \"\"\"\n",
    "        returns : state tuple haing flowstate and n_frame\n",
    "            (flow_state,n_frame)\n",
    "        \"\"\"\n",
    "        flow_state = None\n",
    "\n",
    "        x_mid, y_mid = track.mean[:2]\n",
    "        if boxAndLineOverlap(x_mid, y_mid, (x1_area, y1_area, x2_area, y2_area)):\n",
    "            flow_state = 1\n",
    "        else:\n",
    "            flow_state = 0\n",
    "        return np.array([flow_state,1])\n",
    "\n",
    "    def update_state(self, track_id, state):\n",
    "        \"\"\"\n",
    "        state will contain:\n",
    "        0: object is out of area\n",
    "        1: object is in area\n",
    "        2: object is inflowing\n",
    "        3: object is outflowing\n",
    "\n",
    "        n_frame : frames for which the object will remain inside area at current velocity\n",
    "        will be calculated at each frame\n",
    "\n",
    "        if frame state is zero the n_frame will not be calculated as there is no velocity of the object yet\n",
    "        // [X] in such case box will be used to keep track of the object\n",
    "        we are not tracking the objects that have not been tracked yet... Let them be in the frame uncounted\n",
    "        \"\"\"\n",
    "        # if self.tracks_state is not None:\n",
    "        self.tracks_state[track_id] = state\n",
    "\n",
    "    def get_count(self):\n",
    "        return len(self.tracks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e07e2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_frame = Frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d32d7fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_frame.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b96c26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4b8fadca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0fc801ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "23075a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ca5b4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_frame = Frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "8ab3dc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<deep_sort.deep_sort.track.Track at 0x293e7a510>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fcb8090>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293e75490>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fd05710>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292ed4e50>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28de5b110>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292cd4150>,\n",
       " <deep_sort.deep_sort.track.Track at 0x290b68e50>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fdaccd0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293e792d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x2939abed0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28feafe90>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28ea8fb10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293d670d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x290ad0dd0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293c9c350>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293ec4550>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292e8cd10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292c8d0d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc20f10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x29582bf10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28e935050>]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "205f8803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([0, 0]),\n",
       " 2: array([0, 0]),\n",
       " 3: array([0, 0]),\n",
       " 4: array([0, 0]),\n",
       " 5: array([0, 0]),\n",
       " 6: array([0, 2]),\n",
       " 7: array([0, 0]),\n",
       " 8: array([0, 0]),\n",
       " 9: array([0, 0]),\n",
       " 10: array([0, 0]),\n",
       " 11: array([0, 0]),\n",
       " 12: array([0, 0]),\n",
       " 13: array([0, 0]),\n",
       " 14: array([0, 0]),\n",
       " 15: array([1, 0]),\n",
       " 16: array([0, 0]),\n",
       " 17: array([1, 2]),\n",
       " 19: array([0, 0]),\n",
       " 21: array([0, 0]),\n",
       " 23: array([0, 0]),\n",
       " 24: array([0, 0]),\n",
       " 26: array([0, 0]),\n",
       " 27: array([0, 0]),\n",
       " 29: array([0, 0]),\n",
       " 31: array([0, 0]),\n",
       " 35: array([0, 0]),\n",
       " 36: array([0, 0]),\n",
       " 39: array([0, 0]),\n",
       " 41: array([0, 0]),\n",
       " 43: array([0, 0]),\n",
       " 44: array([0, 0]),\n",
       " 45: array([1, 0]),\n",
       " 48: array([1, 0]),\n",
       " 51: array([1, 2]),\n",
       " 52: array([0, 0]),\n",
       " 53: array([0, 2]),\n",
       " 56: array([0, 0]),\n",
       " 57: array([0, 2]),\n",
       " 58: array([0, 0]),\n",
       " 60: array([0, 0]),\n",
       " 61: array([1, 2]),\n",
       " 63: array([0, 2]),\n",
       " 67: array([0, 0]),\n",
       " 69: array([0, 1]),\n",
       " 70: array([0, 0]),\n",
       " 71: array([0, 0]),\n",
       " 73: array([0, 0]),\n",
       " 75: array([0, 2]),\n",
       " 76: array([1, 0]),\n",
       " 77: array([0, 0]),\n",
       " 79: array([1, 2]),\n",
       " 81: array([1, 2]),\n",
       " 83: array([1, 2]),\n",
       " 84: array([1, 0]),\n",
       " 85: array([0, 0]),\n",
       " 87: array([0, 0]),\n",
       " 89: array([1, 2]),\n",
       " 90: array([0, 2]),\n",
       " 91: array([0, 2]),\n",
       " 93: array([0, 2]),\n",
       " 96: array([0, 0]),\n",
       " 98: array([0, 0]),\n",
       " 101: array([0, 2]),\n",
       " 102: array([0, 2]),\n",
       " 103: array([0, 2]),\n",
       " 105: array([0, 2]),\n",
       " 107: array([0, 2]),\n",
       " 109: array([0, 2])}"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5161b7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.frame_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ac31ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_frame.update(tracker.tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "7de024fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<deep_sort.deep_sort.track.Track at 0x293e7a510>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fcb8090>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293e75490>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fd05710>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292ed4e50>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28de5b110>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292cd4150>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fdaccd0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293e792d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x2939abed0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28feafe90>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28ea8fb10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293d670d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x290ad0dd0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293c9c350>,\n",
       " <deep_sort.deep_sort.track.Track at 0x293ec4550>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292e8cd10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292c8d0d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc20f10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x29582bf10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28e935050>,\n",
       " <deep_sort.deep_sort.track.Track at 0x292c3f290>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2c890>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2ed10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f450>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f510>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f5d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f1d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2f390>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2c350>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2ecd0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc2cf50>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc50f10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc514d0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc50910>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53c90>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53ad0>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53d10>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53410>,\n",
       " <deep_sort.deep_sort.track.Track at 0x28fc53790>,\n",
       " <deep_sort.deep_sort.track.Track at 0x29396ad10>]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "577e1e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([1, 2]),\n",
       " 2: array([1, 2]),\n",
       " 3: array([1, 2]),\n",
       " 4: array([0, 2]),\n",
       " 5: array([1, 2]),\n",
       " 6: array([0, 2]),\n",
       " 7: array([1, 2]),\n",
       " 8: array([1, 2]),\n",
       " 9: array([1, 2]),\n",
       " 10: array([0, 2]),\n",
       " 11: array([0, 2]),\n",
       " 12: array([1, 2]),\n",
       " 13: array([0, 2]),\n",
       " 14: array([0, 2]),\n",
       " 15: array([1, 2]),\n",
       " 16: array([1, 2]),\n",
       " 17: array([1, 2]),\n",
       " 19: array([1, 2]),\n",
       " 21: array([0, 2]),\n",
       " 23: array([1, 2]),\n",
       " 24: array([0, 0]),\n",
       " 26: array([0, 0]),\n",
       " 27: array([0, 0]),\n",
       " 29: array([0, 0]),\n",
       " 31: array([0, 0]),\n",
       " 35: array([0, 0]),\n",
       " 36: array([0, 0]),\n",
       " 39: array([0, 0]),\n",
       " 41: array([0, 0]),\n",
       " 43: array([0, 0]),\n",
       " 44: array([0, 0]),\n",
       " 45: array([1, 0]),\n",
       " 48: array([1, 0]),\n",
       " 51: array([1, 1]),\n",
       " 52: array([0, 0]),\n",
       " 53: array([0, 1]),\n",
       " 56: array([0, 0]),\n",
       " 57: array([0, 1]),\n",
       " 58: array([0, 0]),\n",
       " 60: array([0, 0]),\n",
       " 61: array([1, 1]),\n",
       " 63: array([0, 1]),\n",
       " 67: array([0, 0]),\n",
       " 69: array([0, 0]),\n",
       " 70: array([0, 0]),\n",
       " 71: array([0, 0]),\n",
       " 73: array([0, 0]),\n",
       " 75: array([0, 1]),\n",
       " 76: array([1, 0]),\n",
       " 77: array([0, 0]),\n",
       " 79: array([1, 1]),\n",
       " 81: array([1, 1]),\n",
       " 83: array([1, 1]),\n",
       " 84: array([1, 0]),\n",
       " 85: array([0, 0]),\n",
       " 87: array([0, 0]),\n",
       " 89: array([1, 1]),\n",
       " 90: array([0, 1]),\n",
       " 91: array([0, 1]),\n",
       " 93: array([0, 1]),\n",
       " 96: array([0, 0]),\n",
       " 98: array([0, 0]),\n",
       " 101: array([0, 1]),\n",
       " 102: array([0, 1]),\n",
       " 103: array([0, 1]),\n",
       " 105: array([0, 1]),\n",
       " 107: array([0, 1]),\n",
       " 109: array([0, 1])}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ae956d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.frame_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "bb461991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "prev_frame = copy.deepcopy(cur_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "74e92b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([1, 2]),\n",
       " 2: array([1, 2]),\n",
       " 3: array([1, 2]),\n",
       " 4: array([0, 2]),\n",
       " 5: array([1, 2]),\n",
       " 6: array([0, 2]),\n",
       " 7: array([1, 2]),\n",
       " 8: array([1, 2]),\n",
       " 9: array([1, 2]),\n",
       " 10: array([0, 2]),\n",
       " 11: array([0, 2]),\n",
       " 12: array([1, 2]),\n",
       " 13: array([0, 2]),\n",
       " 14: array([0, 2]),\n",
       " 15: array([1, 2]),\n",
       " 16: array([1, 2]),\n",
       " 17: array([1, 2]),\n",
       " 19: array([1, 2]),\n",
       " 21: array([0, 2]),\n",
       " 23: array([1, 2]),\n",
       " 24: array([0, 0]),\n",
       " 26: array([0, 0]),\n",
       " 27: array([0, 0]),\n",
       " 29: array([0, 0]),\n",
       " 31: array([0, 0]),\n",
       " 35: array([0, 0]),\n",
       " 36: array([0, 0]),\n",
       " 39: array([0, 0]),\n",
       " 41: array([0, 0]),\n",
       " 43: array([0, 0]),\n",
       " 44: array([0, 0]),\n",
       " 45: array([1, 0]),\n",
       " 48: array([1, 0]),\n",
       " 51: array([1, 1]),\n",
       " 52: array([0, 0]),\n",
       " 53: array([0, 1]),\n",
       " 56: array([0, 0]),\n",
       " 57: array([0, 1]),\n",
       " 58: array([0, 0]),\n",
       " 60: array([0, 0]),\n",
       " 61: array([1, 1]),\n",
       " 63: array([0, 1]),\n",
       " 67: array([0, 0]),\n",
       " 69: array([0, 0]),\n",
       " 70: array([0, 0]),\n",
       " 71: array([0, 0]),\n",
       " 73: array([0, 0]),\n",
       " 75: array([0, 1]),\n",
       " 76: array([1, 0]),\n",
       " 77: array([0, 0]),\n",
       " 79: array([1, 1]),\n",
       " 81: array([1, 1]),\n",
       " 83: array([1, 1]),\n",
       " 84: array([1, 0]),\n",
       " 85: array([0, 0]),\n",
       " 87: array([0, 0]),\n",
       " 89: array([1, 1]),\n",
       " 90: array([0, 1]),\n",
       " 91: array([0, 1]),\n",
       " 93: array([0, 1]),\n",
       " 96: array([0, 0]),\n",
       " 98: array([0, 0]),\n",
       " 101: array([0, 1]),\n",
       " 102: array([0, 1]),\n",
       " 103: array([0, 1]),\n",
       " 105: array([0, 1]),\n",
       " 107: array([0, 1]),\n",
       " 109: array([0, 1])}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_frame.tracks_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1a000c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 5 birds, 135.7ms\n",
      "Speed: 1.4ms preprocess, 135.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "ret, frame, detections = defun(cap)\n",
    "tracker.update(frame, detections)\n",
    "cur_frame.update(tracker.tracks)\n",
    "prev_frame = copy.deepcopy(cur_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "43f1dffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    }
   ],
   "source": [
    "print(len(cur_frame.tracks_state),list(set(cur_frame.tracks_state) - set(prev_frame.tracks_state)),list(set(prev_frame.tracks_state) - set(cur_frame.tracks_state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b3961931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 5 birds, 186.7ms\n",
      "Speed: 1.5ms preprocess, 186.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 4 birds, 111.4ms\n",
      "Speed: 1.8ms preprocess, 111.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 35 persons, 2 birds, 122.3ms\n",
      "Speed: 1.5ms preprocess, 122.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 2 birds, 115.8ms\n",
      "Speed: 1.4ms preprocess, 115.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 2 birds, 110.2ms\n",
      "Speed: 1.5ms preprocess, 110.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 33 persons, 2 birds, 166.3ms\n",
      "Speed: 1.6ms preprocess, 166.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 birds, 117.9ms\n",
      "Speed: 1.5ms preprocess, 117.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 32 persons, 3 birds, 119.3ms\n",
      "Speed: 1.4ms preprocess, 119.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 33 persons, 4 birds, 122.5ms\n",
      "Speed: 1.5ms preprocess, 122.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 33 persons, 4 birds, 122.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.4ms preprocess, 122.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 34 persons, 4 birds, 165.1ms\n",
      "Speed: 1.4ms preprocess, 165.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 3 birds, 118.9ms\n",
      "Speed: 1.6ms preprocess, 118.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 2 birds, 119.1ms\n",
      "Speed: 1.4ms preprocess, 119.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 32 persons, 2 birds, 114.8ms\n",
      "Speed: 1.4ms preprocess, 114.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 33 persons, 2 birds, 108.9ms\n",
      "Speed: 1.5ms preprocess, 108.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 31 persons, 2 birds, 1 dog, 127.4ms\n",
      "Speed: 1.5ms preprocess, 127.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 33 persons, 1 bird, 1 dog, 273.6ms\n",
      "Speed: 1.4ms preprocess, 273.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 32 persons, 1 bird, 1 dog, 132.7ms\n",
      "Speed: 1.6ms preprocess, 132.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 bird, 1 dog, 115.8ms\n",
      "Speed: 1.5ms preprocess, 115.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 1 bird, 1 dog, 124.3ms\n",
      "Speed: 1.3ms preprocess, 124.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 1 bird, 1 dog, 118.2ms\n",
      "Speed: 1.4ms preprocess, 118.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 35 persons, 1 bird, 116.4ms\n",
      "Speed: 1.4ms preprocess, 116.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 bird, 118.6ms\n",
      "Speed: 1.5ms preprocess, 118.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 1 bird, 121.0ms\n",
      "Speed: 1.4ms preprocess, 121.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 34 persons, 1 bird, 171.7ms\n",
      "Speed: 1.4ms preprocess, 171.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 birds, 105.0ms\n",
      "Speed: 1.4ms preprocess, 105.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 31 persons, 2 birds, 119.6ms\n",
      "Speed: 1.3ms preprocess, 119.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 31 persons, 3 birds, 176.4ms\n",
      "Speed: 1.5ms preprocess, 176.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 bird, 116.5ms\n",
      "Speed: 1.4ms preprocess, 116.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 31 persons, 2 birds, 122.0ms\n",
      "Speed: 1.5ms preprocess, 122.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 33 persons, 1 bird, 117.4ms\n",
      "Speed: 1.4ms preprocess, 117.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 33 persons, 1 bird, 116.4ms\n",
      "Speed: 1.4ms preprocess, 116.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 34 persons, 2 birds, 160.2ms\n",
      "Speed: 1.5ms preprocess, 160.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 2 birds, 116.3ms\n",
      "Speed: 1.4ms preprocess, 116.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 35 persons, 2 birds, 111.9ms\n",
      "Speed: 1.3ms preprocess, 111.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 32 persons, 2 birds, 114.2ms\n",
      "Speed: 1.5ms preprocess, 114.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 33 persons, 2 birds, 124.8ms\n",
      "Speed: 1.4ms preprocess, 124.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 35 persons, 2 birds, 112.1ms\n",
      "Speed: 1.5ms preprocess, 112.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 35 persons, 2 birds, 124.1ms\n",
      "Speed: 1.4ms preprocess, 124.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 35 persons, 2 birds, 116.6ms\n",
      "Speed: 1.4ms preprocess, 116.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 38 persons, 2 birds, 110.8ms\n",
      "Speed: 1.3ms preprocess, 110.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 36 persons, 1 bird, 126.4ms\n",
      "Speed: 1.6ms preprocess, 126.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 1 bird, 117.5ms\n",
      "Speed: 1.4ms preprocess, 117.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 2 birds, 1 dog, 109.2ms\n",
      "Speed: 1.4ms preprocess, 109.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 1 bird, 1 dog, 117.4ms\n",
      "Speed: 1.3ms preprocess, 117.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 1 bird, 1 dog, 1 cow, 120.0ms\n",
      "Speed: 1.3ms preprocess, 120.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 1 bird, 1 dog, 123.9ms\n",
      "Speed: 1.4ms preprocess, 123.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 1 bird, 1 dog, 112.2ms\n",
      "Speed: 1.4ms preprocess, 112.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 2 birds, 1 dog, 103.7ms\n",
      "Speed: 1.4ms preprocess, 103.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 1 bird, 1 dog, 118.4ms\n",
      "Speed: 1.3ms preprocess, 118.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 1 bird, 118.3ms\n",
      "Speed: 1.3ms preprocess, 118.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 38 persons, 1 bird, 174.2ms\n",
      "Speed: 1.3ms preprocess, 174.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 39 persons, 1 bird, 1 horse, 128.2ms\n",
      "Speed: 1.4ms preprocess, 128.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 1 bird, 116.4ms\n",
      "Speed: 1.4ms preprocess, 116.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 1 bird, 116.7ms\n",
      "Speed: 1.5ms preprocess, 116.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 1 bird, 115.1ms\n",
      "Speed: 1.4ms preprocess, 115.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 41 persons, 1 bird, 113.2ms\n",
      "Speed: 1.3ms preprocess, 113.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 116.3ms\n",
      "Speed: 1.3ms preprocess, 116.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 119.0ms\n",
      "Speed: 1.6ms preprocess, 119.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 1 bird, 102.4ms\n",
      "Speed: 1.3ms preprocess, 102.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 1 bird, 124.5ms\n",
      "Speed: 1.3ms preprocess, 124.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 1 bird, 121.3ms\n",
      "Speed: 1.3ms preprocess, 121.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 39 persons, 1 bird, 1 dog, 147.5ms\n",
      "Speed: 1.3ms preprocess, 147.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 37 persons, 1 bird, 1 dog, 114.8ms\n",
      "Speed: 1.4ms preprocess, 114.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 1 bird, 1 dog, 109.3ms\n",
      "Speed: 1.5ms preprocess, 109.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 1 bird, 1 dog, 128.4ms\n",
      "Speed: 1.4ms preprocess, 128.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 1 bird, 117.8ms\n",
      "Speed: 1.3ms preprocess, 117.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n",
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 1 bird, 121.0ms\n",
      "Speed: 2.0ms preprocess, 121.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 42 persons, 110.4ms\n",
      "Speed: 1.4ms preprocess, 110.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 41 persons, 125.5ms\n",
      "Speed: 1.5ms preprocess, 125.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 40 persons, 1 bird, 110.6ms\n",
      "Speed: 1.5ms preprocess, 110.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 42 persons, 1 bird, 113.8ms\n",
      "Speed: 1.4ms preprocess, 113.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 3 birds, 117.1ms\n",
      "Speed: 1.3ms preprocess, 117.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 38 persons, 3 birds, 124.8ms\n",
      "Speed: 1.4ms preprocess, 124.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 3 birds, 126.8ms\n",
      "Speed: 1.4ms preprocess, 126.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 3 birds, 116.2ms\n",
      "Speed: 1.3ms preprocess, 116.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 3 birds, 113.1ms\n",
      "Speed: 1.4ms preprocess, 113.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 39 persons, 1 bird, 126.0ms\n",
      "Speed: 1.5ms preprocess, 126.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 41 persons, 1 bird, 219.8ms\n",
      "Speed: 1.8ms preprocess, 219.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 40 persons, 2 birds, 119.6ms\n",
      "Speed: 1.4ms preprocess, 119.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 1 horse, 115.7ms\n",
      "Speed: 1.4ms preprocess, 115.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 115.5ms\n",
      "Speed: 1.6ms preprocess, 115.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 36 persons, 123.7ms\n",
      "Speed: 1.5ms preprocess, 123.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 37 persons, 125.4ms\n",
      "Speed: 1.6ms preprocess, 125.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 120.4ms\n",
      "Speed: 1.5ms preprocess, 120.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 41 persons, 114.0ms\n",
      "Speed: 1.4ms preprocess, 114.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 39 persons, 118.4ms\n",
      "Speed: 1.6ms preprocess, 118.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 40 persons, 127.7ms\n",
      "Speed: 1.4ms preprocess, 127.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 38 persons, 1 backpack, 126.9ms\n",
      "Speed: 1.5ms preprocess, 126.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 36 persons, 169.3ms\n",
      "Speed: 1.5ms preprocess, 169.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 119.6ms\n",
      "Speed: 1.3ms preprocess, 119.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 120.5ms\n",
      "Speed: 1.6ms preprocess, 120.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 36 persons, 1 dog, 120.4ms\n",
      "Speed: 1.5ms preprocess, 120.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 116.2ms\n",
      "Speed: 1.4ms preprocess, 116.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 122.9ms\n",
      "Speed: 1.3ms preprocess, 122.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 37 persons, 1 backpack, 119.0ms\n",
      "Speed: 1.3ms preprocess, 119.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 36 persons, 2 backpacks, 134.7ms\n",
      "Speed: 1.4ms preprocess, 134.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 38 persons, 1 bird, 1 backpack, 110.3ms\n",
      "Speed: 1.5ms preprocess, 110.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 35 persons, 1 bird, 124.7ms\n",
      "Speed: 1.4ms preprocess, 124.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 37 persons, 1 bird, 113.7ms\n",
      "Speed: 1.5ms preprocess, 113.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 35 persons, 110.0ms\n",
      "Speed: 1.5ms preprocess, 110.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 35 persons, 1 bird, 117.1ms\n",
      "Speed: 1.6ms preprocess, 117.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 35 persons, 181.4ms\n",
      "Speed: 1.5ms preprocess, 181.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 34 persons, 140.4ms\n",
      "Speed: 1.8ms preprocess, 140.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 bird, 112.8ms\n",
      "Speed: 1.5ms preprocess, 112.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 [] []\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[320], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prev_frame \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(cur_frame)\n\u001b[1;32m      3\u001b[0m ret, frame, detections \u001b[38;5;241m=\u001b[39m defun(cap)\n\u001b[0;32m----> 4\u001b[0m tracker\u001b[38;5;241m.\u001b[39mupdate(frame, detections)\n\u001b[1;32m      5\u001b[0m cur_frame\u001b[38;5;241m.\u001b[39mupdate(tracker\u001b[38;5;241m.\u001b[39mtracks)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cur_frame\u001b[38;5;241m.\u001b[39mtracks_state),\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(cur_frame\u001b[38;5;241m.\u001b[39mtracks_state) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(prev_frame\u001b[38;5;241m.\u001b[39mtracks_state)),\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(prev_frame\u001b[38;5;241m.\u001b[39mtracks_state) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(cur_frame\u001b[38;5;241m.\u001b[39mtracks_state)))\n",
      "Cell \u001b[0;32mIn[287], line 31\u001b[0m, in \u001b[0;36mTracker.update\u001b[0;34m(self, frame, detections)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#         Yolo_confidence_scores = [d[-2] for d in detections]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         scores \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m detections]\n\u001b[0;32m---> 31\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(frame, bboxes)\n\u001b[1;32m     33\u001b[0m         dets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m bbox_id, bbox \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bboxes):\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/deep_sort/tools/generate_detections.py:113\u001b[0m, in \u001b[0;36mcreate_box_encoder.<locals>.encoder\u001b[0;34m(image, boxes)\u001b[0m\n\u001b[1;32m    111\u001b[0m     image_patches\u001b[38;5;241m.\u001b[39mappend(patch)\n\u001b[1;32m    112\u001b[0m image_patches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(image_patches)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_encoder(image_patches, batch_size)\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/deep_sort/tools/generate_detections.py:92\u001b[0m, in \u001b[0;36mImageEncoder.__call__\u001b[0;34m(self, data_x, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     91\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(data_x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_dim), np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 92\u001b[0m     _run_in_batches(\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_var, feed_dict\u001b[38;5;241m=\u001b[39mx),\n\u001b[1;32m     94\u001b[0m         {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_var: data_x}, out, batch_size)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/deep_sort/tools/generate_detections.py:18\u001b[0m, in \u001b[0;36m_run_in_batches\u001b[0;34m(f, data_dict, out, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     s, e \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m batch_size, (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m     17\u001b[0m     batch_data_dict \u001b[38;5;241m=\u001b[39m {k: v[s:e] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 18\u001b[0m     out[s:e] \u001b[38;5;241m=\u001b[39m f(batch_data_dict)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[1;32m     20\u001b[0m     batch_data_dict \u001b[38;5;241m=\u001b[39m {k: v[e:] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/deep_sort/tools/generate_detections.py:93\u001b[0m, in \u001b[0;36mImageEncoder.__call__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     91\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(data_x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_dim), np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     92\u001b[0m     _run_in_batches(\n\u001b[0;32m---> 93\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_var, feed_dict\u001b[38;5;241m=\u001b[39mx),\n\u001b[1;32m     94\u001b[0m         {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_var: data_x}, out, batch_size)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/tensorflow/python/client/session.py:969\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;28;01mNone\u001b[39;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    970\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    971\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    972\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/tensorflow/python/client/session.py:1192\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1192\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_run(handle, final_targets, final_fetches,\n\u001b[1;32m   1193\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1195\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/tensorflow/python/client/session.py:1372\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1372\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m   1373\u001b[0m                        run_metadata)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/tensorflow/python/client/session.py:1379\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1378\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1380\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1381\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/tensorflow/python/client/session.py:1362\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1360\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1362\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1363\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[0;32m~/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/tensorflow/python/client/session.py:1455\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1454\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1455\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf_session\u001b[38;5;241m.\u001b[39mTF_SessionRun_wrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session, options, feed_dict,\n\u001b[1;32m   1456\u001b[0m                                           fetch_list, target_list,\n\u001b[1;32m   1457\u001b[0m                                           run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while ret and len(list(set(prev_frame.tracks_state) - set(cur_frame.tracks_state)))==0:\n",
    "    prev_frame = copy.deepcopy(cur_frame)\n",
    "    ret, frame, detections = defun(cap)\n",
    "    tracker.update(frame, detections)\n",
    "    cur_frame.update(tracker.tracks)\n",
    "    print(len(cur_frame.tracks_state),list(set(cur_frame.tracks_state) - set(prev_frame.tracks_state)),list(set(prev_frame.tracks_state) - set(cur_frame.tracks_state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "da771af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c1f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6bb55639",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_frame = copy.deepcopy(cur_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8c5dd690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([1, 2]),\n",
       " 2: array([1, 2]),\n",
       " 3: array([1, 2]),\n",
       " 4: array([0, 2]),\n",
       " 5: array([1, 2]),\n",
       " 6: array([0, 2]),\n",
       " 7: array([1, 2]),\n",
       " 8: array([1, 2]),\n",
       " 9: array([1, 2]),\n",
       " 10: array([0, 2]),\n",
       " 11: array([0, 2]),\n",
       " 12: array([1, 2]),\n",
       " 13: array([0, 2]),\n",
       " 14: array([0, 2]),\n",
       " 15: array([1, 2]),\n",
       " 16: array([1, 2]),\n",
       " 17: array([1, 2]),\n",
       " 19: array([1, 2]),\n",
       " 21: array([0, 2]),\n",
       " 23: array([1, 2]),\n",
       " 24: array([1, 2])}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_frame.tracks_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "727f8ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([1, 2]),\n",
       " 2: array([1, 2]),\n",
       " 3: array([1, 2]),\n",
       " 4: array([0, 2]),\n",
       " 5: array([1, 2]),\n",
       " 6: array([0, 2]),\n",
       " 7: array([1, 2]),\n",
       " 8: array([1, 2]),\n",
       " 9: array([1, 2]),\n",
       " 10: array([0, 2]),\n",
       " 11: array([0, 2]),\n",
       " 12: array([1, 2]),\n",
       " 13: array([0, 2]),\n",
       " 14: array([0, 2]),\n",
       " 15: array([1, 2]),\n",
       " 16: array([1, 2]),\n",
       " 17: array([1, 2]),\n",
       " 19: array([1, 2]),\n",
       " 21: array([0, 2]),\n",
       " 23: array([1, 2]),\n",
       " 24: array([1, 2]),\n",
       " 26: array([0, 2]),\n",
       " 27: array([0, 2])}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cbd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
