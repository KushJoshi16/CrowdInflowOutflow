{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3962b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Frame:\n",
    "    \"\"\"\n",
    "    This class is used to store the state of the frame\n",
    "    State 0 means that the frame is not processed\n",
    "    State 1 means that the frame is processed\n",
    "    if the frame is not processed there wont be any tracks and only boxes will be present \n",
    "    and we will store the detections in place of tracks\n",
    "\n",
    "\n",
    "    if any object goes undetected in current frame then \n",
    "    the last tracks_status of the object can give us the number of frames required for object to be considered as lost out of area\n",
    "    once the object is lost we no more need to track it and count it\n",
    "    but if the object is not lost it should be kept in counting if it comes back in tracking and is within area.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    frame_state = None\n",
    "    tracks = None\n",
    "    tracks_state = None\n",
    "\n",
    "\n",
    "    def __init__(self, frame_state = 0, prev_frame = None):\n",
    "        self.frame_state = frame_state\n",
    "        self.tracks = []\n",
    "        self.tracks_state = dict()\n",
    "    \n",
    "    def append(self, track):\n",
    "        self.tracks.append(track)\n",
    "    \n",
    "    def update(self, tracks):\n",
    "        self.tracks = tracks\n",
    "#         untracked_tracks = list(set(self.tracks) - set(tracks))\n",
    "#         updated_tracks = []\n",
    "        \n",
    "#         for track in untracked_tracks:\n",
    "#             self.tracks_state[track.track_id][1] -= 1\n",
    "            \n",
    "#             if self.tracks_state[track.track_id][1] <= 0:\n",
    "#                 self.tracks.remove(track)\n",
    "                \n",
    "#         for track in tracks:\n",
    "#             if track not in self.tracks:\n",
    "#                 self.append(track)\n",
    "#             state = self.calculate_state(track)\n",
    "#             self.update_state(track.track_id, state)\n",
    "        \n",
    "#         for track in self.tracks:\n",
    "            \n",
    "            \n",
    "#         self.tracks = [track for track in self.tracks if self.tracks_state[track.track_id][1] <= 0]\n",
    "            \n",
    "\n",
    "\n",
    "    def calculate_state(self, track):\n",
    "        \"\"\"\n",
    "        returns : state tuple haing flowstate and n_frame\n",
    "            (flow_state,n_frame)\n",
    "        \"\"\"\n",
    "        flow_state = None\n",
    "\n",
    "        x_mid, y_mid = track.mean[:2]\n",
    "        if boxAndAreaOverlap(x_mid, y_mid, (x1_area, y1_area, x2_area, y2_area)):\n",
    "            flow_state = 1\n",
    "        else:\n",
    "            flow_state = 0\n",
    "        return np.array([flow_state,1])\n",
    "\n",
    "    def update_state(self, track_id, state):\n",
    "        \"\"\"\n",
    "        state will contain:\n",
    "        0: object is out of area\n",
    "        1: object is in area\n",
    "        2: object is inflowing\n",
    "        3: object is outflowing\n",
    "\n",
    "        n_frame : frames for which the object will remain inside area at current velocity\n",
    "        will be calculated at each frame\n",
    "\n",
    "        if frame state is zero the n_frame will not be calculated as there is no velocity of the object yet\n",
    "        // [X] in such case box will be used to keep track of the object\n",
    "        we are not tracking the objects that have not been tracked yet... Let them be in the frame uncounted\n",
    "        \"\"\"\n",
    "        # if self.tracks_state is not None:\n",
    "        self.tracks_state[track_id] = state\n",
    "\n",
    "    def get_count(self):\n",
    "        return len(self.tracks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec5eec",
   "metadata": {},
   "source": [
    "## mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6b4a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from tracker import Tracker\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d874b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = './assets/im1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4cf0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxAndLineOverlap(x_mid_point, y_mid_point, line_coordinates):\n",
    "    x1_line, y1_line, x2_line, y2_line = line_coordinates #Unpacking\n",
    "\n",
    "    if (x_mid_point >= x1_line and x_mid_point <= x2_line+5) and\\\n",
    "        (y_mid_point >= y1_line and y_mid_point <= y2_line+5):\n",
    "        return True\n",
    "    return False\n",
    "def boxAndAreaOverlap(x_mid_point, y_mid_point, line_coordinates):\n",
    "    x1_line, y1_line, x2_line, y2_line = line_coordinates #Unpacking\n",
    "\n",
    "    if (x_mid_point >= x1_line and x_mid_point <= x2_line) and\\\n",
    "        (y_mid_point >= y1_line and y_mid_point <= y2_line):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34ac346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(os.getcwd(), 'assets', 'my_vid.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "x1_area = int(0.10 * video_width)\n",
    "y1_area = int(0.10 * video_height)\n",
    "x2_area = int(0.90 * video_width)\n",
    "y2_area = int(0.90 * video_height)\n",
    "\n",
    "detections = []\n",
    "def defun(cap):\n",
    "\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    results = model(frame)\n",
    "    \n",
    "    detections = []\n",
    "    for result in results:\n",
    "        for r in result.boxes.data.tolist():\n",
    "            x1, y1, x2, y2, score, class_id = r\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "            score = float(score)\n",
    "            class_id = int(class_id)\n",
    "\n",
    "            if score > 0.5 and class_id == 0:\n",
    "                detections.append([x1, y1, x2, y2, score, class_id])\n",
    "    return (ret, frame, detections)\n",
    "# for i in range(5):\n",
    "#     defun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df0bf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6995460",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "682b5b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 bottle, 1 chair, 1 tv, 144.4ms\n",
      "Speed: 2.7ms preprocess, 144.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 chair, 1 tv, 187.1ms\n",
      "Speed: 1.9ms preprocess, 187.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 chair, 1 tv, 119.7ms\n",
      "Speed: 1.9ms preprocess, 119.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 chair, 1 tv, 110.4ms\n",
      "Speed: 2.0ms preprocess, 110.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 chair, 1 tv, 120.7ms\n",
      "Speed: 2.3ms preprocess, 120.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 119.2ms\n",
      "Speed: 2.0ms preprocess, 119.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 120.3ms\n",
      "Speed: 1.7ms preprocess, 120.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 122.7ms\n",
      "Speed: 1.8ms preprocess, 122.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 114.9ms\n",
      "Speed: 1.7ms preprocess, 114.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 116.1ms\n",
      "Speed: 1.8ms preprocess, 116.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 112.3ms\n",
      "Speed: 1.9ms preprocess, 112.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 121.3ms\n",
      "Speed: 1.9ms preprocess, 121.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 149.5ms\n",
      "Speed: 1.9ms preprocess, 149.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 184.8ms\n",
      "Speed: 1.9ms preprocess, 184.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 120.4ms\n",
      "Speed: 2.0ms preprocess, 120.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 112.0ms\n",
      "Speed: 1.9ms preprocess, 112.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 121.0ms\n",
      "Speed: 1.8ms preprocess, 121.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 131.1ms\n",
      "Speed: 1.8ms preprocess, 131.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 157.5ms\n",
      "Speed: 2.0ms preprocess, 157.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 124.2ms\n",
      "Speed: 1.9ms preprocess, 124.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 123.4ms\n",
      "Speed: 1.9ms preprocess, 123.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 131.7ms\n",
      "Speed: 2.0ms preprocess, 131.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 180.8ms\n",
      "Speed: 2.0ms preprocess, 180.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 123.3ms\n",
      "Speed: 2.1ms preprocess, 123.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 130.0ms\n",
      "Speed: 2.0ms preprocess, 130.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 116.5ms\n",
      "Speed: 1.9ms preprocess, 116.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 136.7ms\n",
      "Speed: 1.7ms preprocess, 136.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 119.1ms\n",
      "Speed: 1.7ms preprocess, 119.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 126.3ms\n",
      "Speed: 1.7ms preprocess, 126.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 119.2ms\n",
      "Speed: 1.8ms preprocess, 119.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 166.8ms\n",
      "Speed: 1.6ms preprocess, 166.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 116.9ms\n",
      "Speed: 2.2ms preprocess, 116.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 122.2ms\n",
      "Speed: 1.9ms preprocess, 122.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 109.4ms\n",
      "Speed: 1.8ms preprocess, 109.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 111.3ms\n",
      "Speed: 2.4ms preprocess, 111.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 125.0ms\n",
      "Speed: 1.9ms preprocess, 125.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 109.0ms\n",
      "Speed: 1.8ms preprocess, 109.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 118.8ms\n",
      "Speed: 2.0ms preprocess, 118.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 115.1ms\n",
      "Speed: 2.0ms preprocess, 115.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 178.6ms\n",
      "Speed: 1.8ms preprocess, 178.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 116.7ms\n",
      "Speed: 2.0ms preprocess, 116.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 127.2ms\n",
      "Speed: 1.7ms preprocess, 127.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 110.8ms\n",
      "Speed: 1.7ms preprocess, 110.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 126.2ms\n",
      "Speed: 1.8ms preprocess, 126.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 123.7ms\n",
      "Speed: 1.9ms preprocess, 123.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 126.1ms\n",
      "Speed: 1.9ms preprocess, 126.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 125.9ms\n",
      "Speed: 2.0ms preprocess, 125.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 116.2ms\n",
      "Speed: 1.9ms preprocess, 116.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 175.6ms\n",
      "Speed: 1.9ms preprocess, 175.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 125.6ms\n",
      "Speed: 2.0ms preprocess, 125.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 136.5ms\n",
      "Speed: 1.6ms preprocess, 136.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 116.5ms\n",
      "Speed: 1.8ms preprocess, 116.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 123.2ms\n",
      "Speed: 1.7ms preprocess, 123.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 117.7ms\n",
      "Speed: 1.7ms preprocess, 117.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 123.2ms\n",
      "Speed: 2.0ms preprocess, 123.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 122.2ms\n",
      "Speed: 1.9ms preprocess, 122.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 140.1ms\n",
      "Speed: 2.0ms preprocess, 140.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 138.2ms\n",
      "Speed: 2.0ms preprocess, 138.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 125.9ms\n",
      "Speed: 2.1ms preprocess, 125.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 112.0ms\n",
      "Speed: 2.2ms preprocess, 112.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 chair, 1 tv, 122.5ms\n",
      "Speed: 1.9ms preprocess, 122.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 chair, 1 tv, 172.9ms\n",
      "Speed: 2.1ms preprocess, 172.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 113.3ms\n",
      "Speed: 2.4ms preprocess, 113.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 133.8ms\n",
      "Speed: 2.1ms preprocess, 133.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 117.3ms\n",
      "Speed: 2.0ms preprocess, 117.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 124.3ms\n",
      "Speed: 1.7ms preprocess, 124.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 170.5ms\n",
      "Speed: 1.8ms preprocess, 170.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 116.6ms\n",
      "Speed: 2.4ms preprocess, 116.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 132.9ms\n",
      "Speed: 1.9ms preprocess, 132.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 117.6ms\n",
      "Speed: 2.0ms preprocess, 117.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 126.8ms\n",
      "Speed: 1.9ms preprocess, 126.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 124.0ms\n",
      "Speed: 2.1ms preprocess, 124.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 126.0ms\n",
      "Speed: 1.8ms preprocess, 126.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 121.3ms\n",
      "Speed: 2.0ms preprocess, 121.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 1 tv, 108.7ms\n",
      "Speed: 1.8ms preprocess, 108.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 166.8ms\n",
      "Speed: 1.8ms preprocess, 166.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 121.0ms\n",
      "Speed: 2.0ms preprocess, 121.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 1 tv, 119.9ms\n",
      "Speed: 1.9ms preprocess, 119.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 1 tv, 119.1ms\n",
      "Speed: 1.9ms preprocess, 119.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 1 tv, 116.7ms\n",
      "Speed: 1.9ms preprocess, 116.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 120.3ms\n",
      "Speed: 1.8ms preprocess, 120.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 1 tv, 120.6ms\n",
      "Speed: 2.0ms preprocess, 120.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 1 tv, 115.9ms\n",
      "Speed: 2.1ms preprocess, 115.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 123.5ms\n",
      "Speed: 1.8ms preprocess, 123.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 171.9ms\n",
      "Speed: 2.0ms preprocess, 171.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 119.2ms\n",
      "Speed: 1.9ms preprocess, 119.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 134.7ms\n",
      "Speed: 1.8ms preprocess, 134.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 136.3ms\n",
      "Speed: 1.9ms preprocess, 136.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 122.7ms\n",
      "Speed: 1.8ms preprocess, 122.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 couch, 177.7ms\n",
      "Speed: 1.8ms preprocess, 177.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 118.1ms\n",
      "Speed: 1.7ms preprocess, 118.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 117.9ms\n",
      "Speed: 1.6ms preprocess, 117.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 106.4ms\n",
      "Speed: 1.7ms preprocess, 106.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 169.5ms\n",
      "Speed: 1.6ms preprocess, 169.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 130.6ms\n",
      "Speed: 2.1ms preprocess, 130.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 113.7ms\n",
      "Speed: 1.8ms preprocess, 113.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 119.0ms\n",
      "Speed: 1.9ms preprocess, 119.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 134.9ms\n",
      "Speed: 1.8ms preprocess, 134.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 2 chairs, 1 tv, 114.2ms\n",
      "Speed: 1.8ms preprocess, 114.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 couch, 115.5ms\n",
      "Speed: 1.8ms preprocess, 115.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 couch, 163.0ms\n",
      "Speed: 1.8ms preprocess, 163.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 couch, 119.8ms\n",
      "Speed: 1.7ms preprocess, 119.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 159.9ms\n",
      "Speed: 2.3ms preprocess, 159.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 125.3ms\n",
      "Speed: 2.2ms preprocess, 125.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/Users/kushjoshi/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /Users/kushjoshi/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/ultralytics/assets/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 144.9ms\n",
      "image 2/2 /Users/kushjoshi/6_sem_mid/Mini_Project/Project/TFenv/lib/python3.11/site-packages/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 182.2ms\n",
      "Speed: 2.0ms preprocess, 163.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "ret, frame, detections = defun(cap)\n",
    "while ret:\n",
    "    tracker.update(frame, detections)\n",
    "    l.append(len(tracker.tracks))\n",
    "    ret, frame, detections = defun(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "539ea9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15c5022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_frame = Frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58f76d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09f8cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 143.6ms\n",
      "Speed: 2.7ms preprocess, 143.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 couch, 116.8ms\n",
      "Speed: 2.0ms preprocess, 116.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 bottle, 1 tv, 167.1ms\n",
      "Speed: 1.9ms preprocess, 167.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 bottle, 1 tv, 393.6ms\n",
      "Speed: 1.9ms preprocess, 393.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "[] {1: array([1, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 bottle, 1 tv, 153.0ms\n",
      "Speed: 2.4ms preprocess, 153.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 tv, 114.2ms\n",
      "Speed: 2.0ms preprocess, 114.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] {1: array([1, 0])}\n",
      "[] {1: array([1, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 bottle, 1 tv, 119.5ms\n",
      "Speed: 2.1ms preprocess, 119.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] {1: array([1, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 127.4ms\n",
      "Speed: 2.7ms preprocess, 127.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 couch, 126.8ms\n",
      "Speed: 1.7ms preprocess, 126.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 couch, 153.2ms\n",
      "Speed: 1.6ms preprocess, 153.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 116.1ms\n",
      "Speed: 1.7ms preprocess, 116.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 109.5ms\n",
      "Speed: 1.6ms preprocess, 109.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 117.2ms\n",
      "Speed: 1.9ms preprocess, 117.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 couch, 108.8ms\n",
      "Speed: 1.8ms preprocess, 108.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 couch, 130.1ms\n",
      "Speed: 1.8ms preprocess, 130.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 couch, 120.4ms\n",
      "Speed: 2.0ms preprocess, 120.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 108.6ms\n",
      "Speed: 1.9ms preprocess, 108.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 couch, 111.6ms\n",
      "Speed: 1.6ms preprocess, 111.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 couch, 160.5ms\n",
      "Speed: 1.7ms preprocess, 160.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 couch, 123.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.8ms preprocess, 123.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<deep_sort.deep_sort.track.Track object at 0x2dbd001d0>] {1: array([1, 1])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    prev_frame = copy.deepcopy(cur_frame)\n",
    "    ret, frame, detections = defun(cap)\n",
    "    tracker.update(frame, detections)\n",
    "    cur_frame.update(tracker.tracks)\n",
    "    print(cur_frame.tracks,cur_frame.tracks_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d214192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<deep_sort.deep_sort.track.Track at 0x2dbd001d0>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c9f91fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([1, 1])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_frame.tracks_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e2474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
